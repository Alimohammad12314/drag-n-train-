# import pandas as pd
# import base64
# import io
# import os
# import zipfile
# import numpy as np
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel, Field
# from typing import List, Dict, Any, Optional
# from fastapi.responses import StreamingResponse

# from fastapi.middleware.cors import CORSMiddleware
# import google.generativeai as genai
# from fpdf import FPDF
# import joblib

# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
# from xgboost import XGBClassifier, XGBRegressor
# from sklearn.metrics import (r2_score, mean_squared_error, mean_absolute_error, 
#                              accuracy_score, f1_score, precision_score, recall_score, confusion_matrix)
# from sklearn.preprocessing import OneHotEncoder
# from sklearn.compose import ColumnTransformer
# from sklearn.pipeline import Pipeline
# import matplotlib.pyplot as plt
# import matplotlib
# matplotlib.use('Agg')
# import seaborn as sns

# try:
#     genai.configure(api_key=os.environ["GEMINI_API_KEY"])
#     gemini_model = genai.GenerativeModel('gemini-1.5-flash')
#     print("✅ Gemini API configured successfully.")
# except Exception as e:
#     print(f"⚠️ WARNING: Could not configure Gemini API. AI features will not work. Details: {e}")
#     gemini_model = None

# explanation_cache = {}
# last_trained_model = {"model": None, "features": []}

# class Visualizations(BaseModel):
#     confusion_matrix_plot: Optional[str] = None; actual_vs_predicted_plot: Optional[str] = None
# class FeatureImportance(BaseModel):
#     feature: str; importance: float
# class TrainResponse(BaseModel):
#     status: str; model_name: str; metrics: Dict[str, float]
#     visualizations: Visualizations; feature_importances: Optional[List[FeatureImportance]] = None; features: List[str]
#     generated_code: Optional[str] = None
# class TrainRequest(BaseModel):
#     dataset: List[Dict[str, Any]]; target_column: str; model_name: str
# class SummarizeRequest(BaseModel):
#     model_name: str; target_column: str; metrics: Dict[str, Any]
# class ExplainRequest(BaseModel):
#     term: str
# class SuggestRequest(BaseModel):
#     columns: List[str]; objective: Optional[str] = None
# class ProfileRequest(BaseModel):
#     dataset: List[Dict[str, Any]]
# class ProfileResponse(BaseModel):
#     profile: Dict[str, Dict[str, Any]]
# class PredictRequest(BaseModel):
#     data: Dict[str, Any]
# class ReportRequest(BaseModel):
#     model_name: str
#     metrics: Dict[str, float]
#     summary: str
#     visualizations: Dict[str, Optional[str]]

# app = FastAPI(title="Drag n' Train AI API", version="8.0.0")
# app.add_middleware(CORSMiddleware, allow_origins=["http://localhost:8001", "https://drag-n-train.vercel.app"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# REGRESSION_MODELS = { "Linear Regression": LinearRegression, "Ridge": Ridge, "Lasso": Lasso, "Random Forest Regressor": RandomForestRegressor, "XGBoost Regressor": XGBRegressor }
# CLASSIFICATION_MODELS = { "Logistic Regression": LogisticRegression, "Decision Tree Classifier": DecisionTreeClassifier, "K-Nearest Neighbors": KNeighborsClassifier, "Random Forest Classifier": RandomForestClassifier, "XGBoost Classifier": XGBClassifier }
# ALL_MODELS = {**REGRESSION_MODELS, **CLASSIFICATION_MODELS}

# def generate_python_code(model_name: str, target_column: str, features: List[str], model_class_str: str):
#     model_class_name = model_class_str.rsplit('.', 1)[1]
#     evaluation_metric = "r2_score" if "Regressor" in model_name else "accuracy_score"
#     return f"""# Code generated by Drag n' Train 🚀
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from {model_class_str.rsplit('.', 1)[0]} import {model_class_name}
# from sklearn.metrics import {evaluation_metric}

# # 1. Load your data (replace with your file path)
# # df = pd.read_csv('your_data.csv')
# # --- Assuming 'df' is your loaded DataFrame ---
# # 2. Define Features (X) and Target (y)
# features = {features}
# target = '{target_column}'
# X = df[features]
# y = df[target]
# # 3. Train the '{model_name}' model
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# model = {model_class_name}()
# model.fit(X_train, y_train)
# # 4. Evaluate the model
# predictions = model.predict(X_test)
# score = {evaluation_metric}(y_test, predictions)
# print(f"Model: {model_name}")
# print(f"Score ({{score:.4f}})")
# """

# @app.get("/models", summary="Get suggested models")
# async def get_models(type: str):
#     if type == "regression": return {"models": list(REGRESSION_MODELS.keys())}
#     if type == "classification": return {"models": list(CLASSIFICATION_MODELS.keys())}
#     raise HTTPException(status_code=400, detail="Invalid problem type.")

# @app.post("/train", response_model=TrainResponse, summary="Train model")
# async def train_model(request: TrainRequest):
#     global last_trained_model
#     if request.model_name not in ALL_MODELS: raise HTTPException(status_code=400, detail=f"Model '{request.model_name}' not found.")
#     df = pd.DataFrame(request.dataset)
#     X = df.drop(columns=[request.target_column]); y = df[request.target_column]
#     features_list = X.columns.tolist()
#     numeric_features = X.select_dtypes(include=['number']).columns
#     categorical_features = X.select_dtypes(include=['object', 'category']).columns
#     preprocessor = ColumnTransformer(transformers=[('num', 'passthrough', numeric_features), ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)])
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#     model_class = ALL_MODELS[request.model_name]
#     model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_class())])
#     model_pipeline.fit(X_train, y_train); y_pred = model_pipeline.predict(X_test)
#     last_trained_model["model"] = model_pipeline; last_trained_model["features"] = features_list
#     model_instance = model_pipeline.named_steps['model']
#     model_class_str = model_instance.__class__.__module__ + "." + model_instance.__class__.__name__
#     generated_code = generate_python_code(request.model_name, request.target_column, features_list, model_class_str)
#     metrics = {}; visuals = {}; feature_importances = None
#     def plot_to_base64(fig):
#         buf = io.BytesIO(); fig.savefig(buf, format="png", bbox_inches='tight'); buf.seek(0)
#         return base64.b64encode(buf.getvalue()).decode('utf-8')
    
#     if request.model_name in REGRESSION_MODELS:
#         mse = mean_squared_error(y_test, y_pred)
#         metrics["r2_score"] = round(r2_score(y_test, y_pred), 4)
#         metrics["root_mean_squared_error"] = round(np.sqrt(mse), 4)
#         metrics["mean_absolute_error"] = round(mean_absolute_error(y_test, y_pred), 4)
        
#         fig, ax = plt.subplots(); ax.scatter(y_test, y_pred, alpha=0.7); ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)
#         ax.set_xlabel("Actual Values"); ax.set_ylabel("Predicted Values"); ax.set_title("Actual vs. Predicted")
#         visuals["actual_vs_predicted_plot"] = plot_to_base64(fig); plt.close(fig)
#     elif request.model_name in CLASSIFICATION_MODELS:
#         metrics["accuracy_score"] = round(accuracy_score(y_test, y_pred), 4); metrics["f1_score"] = round(f1_score(y_test, y_pred, average='weighted'), 4)
#         metrics["precision"] = round(precision_score(y_test, y_pred, average='weighted', zero_division=0), 4); metrics["recall"] = round(recall_score(y_test, y_pred, average='weighted', zero_division=0), 4)
#         cm = confusion_matrix(y_test, y_pred); labels = sorted(list(y_test.unique()))
#         fig, ax = plt.subplots(); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=labels, yticklabels=labels)
#         ax.set_xlabel('Predicted Labels'); ax.set_ylabel('True Labels'); ax.set_title('Confusion Matrix')
#         visuals["confusion_matrix_plot"] = plot_to_base64(fig); plt.close(fig)
    
#     if hasattr(model_pipeline.named_steps['model'], 'feature_importances_'):
#         ohe_features = []
#         if 'cat' in model_pipeline.named_steps['preprocessor'].named_transformers_ and hasattr(model_pipeline.named_steps['preprocessor'].named_transformers_['cat'], 'get_feature_names_out'):
#             if model_pipeline.named_steps['preprocessor'].named_transformers_['cat'].n_features_in_ > 0:
#                 ohe_features = model_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()
#         feature_names = numeric_features.tolist() + ohe_features
#         importances = model_pipeline.named_steps['model'].feature_importances_
#         feature_importances = [{"feature": f, "importance": round(float(i), 4)} for f, i in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)]

#     return TrainResponse(status="training_complete", model_name=request.model_name, metrics=metrics, visualizations=visuals, feature_importances=feature_importances, features=features_list, generated_code=generated_code)

# @app.post("/summarize", summary="Generate a simple summary of model results")
# async def summarize_results(request: SummarizeRequest):
#     if not gemini_model: return {"summary": "Gemini API not configured."}
#     metrics_str = ", ".join([f"{k.replace('_', ' ')} of {v}" for k, v in request.metrics.items()])
#     prompt = f"Analyze the performance of a '{request.model_name}' model that predicted '{request.target_column}'. The key metrics are: {metrics_str}. 1. Start with a one-sentence summary of the model's overall performance for a non-technical user. 2. Identify the single most important metric from the list and explain what it means in this specific context (e.g., what does an accuracy of 0.85 mean for predicting the target?). 3. Provide a short, actionable insight or next step. For example, 'This is a good starting point,' or 'This model seems to struggle with...' Keep the entire response to 3-4 sentences."
#     try:
#         response = gemini_model.generate_content(prompt); return {"summary": response.text}
#     except Exception as e:
#         return {"summary": f"Could not generate summary: {e}"}

# @app.post("/explain", summary="Explain an ML term using Gemini")
# async def explain_term(request: ExplainRequest):
#     term = request.term.strip()
#     if term in explanation_cache: return {"explanation": explanation_cache[term]}
#     if not gemini_model: return {"explanation": "Gemini API not configured."}
#     prompt = f"Explain the machine learning term '{term}' in two simple sentences for a complete beginner. Use a real-world analogy to make it clear (e.g., 'Precision is like a cautious archer...'). Do not use technical jargon."
#     try:
#         response = gemini_model.generate_content(prompt); explanation_cache[term] = response.text; return {"explanation": response.text}
#     except Exception as e:
#         return {"explanation": f"Could not get explanation: {e}"}

# @app.post("/suggest_model", summary="Suggest a model based on dataset columns")
# async def suggest_model(request: SuggestRequest):
#     if not gemini_model: return {"suggestion": "Gemini API not configured."}
#     columns_str = ", ".join(request.columns); available_models = ", ".join(list(ALL_MODELS.keys()))
#     user_objective_prompt = f"The user's objective is: '{request.objective}'." if request.objective else "The user has not specified an objective."
#     prompt = f"As an expert data scientist, analyze these dataset columns: {columns_str}. {user_objective_prompt} 1. **Task Recommendation:** Based on the columns and user objective, is this a 'Regression' or 'Classification' task? 2. **Target Suggestion:** Which column is the most likely target variable? 3. **Model Suggestions:** From this list of available models ({available_models}), which are the top 2 you would recommend trying first for this problem? 4. **Reasoning:** Provide a brief, simple justification for your recommendations. Format the response as a single, well-written paragraph."
#     try:
#         response = gemini_model.generate_content(prompt); return {"suggestion": response.text}
#     except Exception as e:
#         return {"suggestion": f"Could not generate suggestion: {e}"}

# @app.post("/profile", response_model=ProfileResponse, summary="Generate a basic profile of the dataset")
# async def profile_dataset(request: ProfileRequest):
#     try:
#         df = pd.DataFrame(request.dataset); profile = {}
#         for col in df.columns:
#             stats = {}; col_data = df[col].dropna()
#             if pd.api.types.is_numeric_dtype(col_data):
#                 stats['type'] = 'Numeric'; stats['mean'] = float(round(col_data.mean(), 2)); stats['median'] = float(round(col_data.median(), 2))
#                 stats['std_dev'] = float(round(col_data.std(), 2)); stats['min'] = float(round(col_data.min(), 2)); stats['max'] = float(round(col_data.max(), 2))
#             else:
#                 stats['type'] = 'Categorical'; stats['unique_values'] = int(col_data.nunique())
#                 stats['top_value'] = col_data.mode().iloc[0] if not col_data.mode().empty else 'N/A'
#             profile[col] = stats
#         return ProfileResponse(profile=profile)
#     except Exception as e:
#         raise HTTPException(status_code=400, detail=f"Could not profile dataset: {e}")

# @app.post("/predict", summary="Make a prediction using the last trained model")
# async def predict(request: PredictRequest):
#     if not last_trained_model["model"]: raise HTTPException(status_code=400, detail="No model has been trained yet.")
#     try:
#         input_df = pd.DataFrame([request.data], columns=last_trained_model["features"])
#         prediction = last_trained_model["model"].predict(input_df)
#         prediction_value = prediction[0]
#         if hasattr(prediction_value, 'item'): prediction_value = prediction_value.item()
#         return {"prediction": prediction_value}
#     except Exception as e:
#         raise HTTPException(status_code=400, detail=f"Could not make a prediction: {e}")

# @app.post("/generate_report", summary="Generate a PDF report of the results")
# async def generate_report(request: ReportRequest):
#     try:
#         pdf = FPDF(); pdf.add_page(); pdf.set_font("Helvetica", "B", 16)
#         pdf.cell(0, 10, "Drag n' Train Analysis Report", 0, 1, 'C'); pdf.ln(10)
#         pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, f"Model: {request.model_name}", 0, 1); pdf.ln(5)
#         pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, "Analysis", 0, 1)
#         pdf.set_font("Helvetica", "", 10); pdf.multi_cell(0, 5, request.summary.encode('latin-1', 'replace').decode('latin-1')); pdf.ln(5)
#         pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, "Performance Metrics", 0, 1)
#         pdf.set_font("Helvetica", "", 10); pdf.cell(95, 10, "Metric", 1, 0, 'C'); pdf.cell(95, 10, "Value", 1, 1, 'C')
#         for key, val in request.metrics.items():
#             pdf.cell(95, 10, key.replace('_', ' ').title(), 1, 0); pdf.cell(95, 10, str(val), 1, 1)
#         pdf.ln(10)
#         for name, b64_str in request.visualizations.items():
#             if b64_str:
#                 pdf.add_page(); pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, name.replace('_', ' ').title(), 0, 1)
#                 img_data = base64.b64decode(b64_str); img_file = io.BytesIO(img_data)
#                 pdf.image(img_file, x=10, y=None, w=190)
#         pdf_buffer = io.BytesIO(); pdf_buffer.write(pdf.output()); pdf_buffer.seek(0)
#         return StreamingResponse(pdf_buffer, media_type="application/pdf", headers={"Content-Disposition": "attachment;filename=drag_n_train_report.pdf"})
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"Failed to generate PDF report: {e}")

# @app.get("/package_model", summary="Package the last trained model as a FastAPI microservice")
# async def package_model():
#     if not last_trained_model.get("model"):
#         raise HTTPException(status_code=400, detail="No model has been trained yet to package.")
#     model_pipeline = last_trained_model["model"]
#     features = last_trained_model["features"]
#     model_buffer = io.BytesIO()
#     joblib.dump(model_pipeline, model_buffer)
#     model_buffer.seek(0)
#     app_template = f"""
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# import joblib
# import pandas as pd
# from typing import Dict, Any

# model = joblib.load('model.pkl')
# app = FastAPI(title="Drag n' Train Deployed Model")
# class PredictionRequest(BaseModel):
#     data: Dict[str, Any]
# @app.post("/predict")
# def predict(request: PredictionRequest):
#     try:
#         input_df = pd.DataFrame([request.data], columns={features})
#         prediction = model.predict(input_df)
#         prediction_value = prediction[0]
#         if hasattr(prediction_value, 'item'):
#             prediction_value = prediction_value.item()
#         return {{"prediction": prediction_value}}
#     except Exception as e:
#         raise HTTPException(status_code=400, detail=str(e))
# @app.get("/")
# def read_root():
#     return {{"message": "Model API is running. POST to /predict with your data."}}
# """
#     requirements = f"""fastapi\nuvicorn[standard]\nsklearn\npandas\njoblib\nxgboost\n"""
#     zip_buffer = io.BytesIO()
#     with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
#         zf.writestr("model.pkl", model_buffer.read())
#         zf.writestr("app.py", app_template)
#         zf.writestr("requirements.txt", requirements)
#         zf.writestr("README.md", "# Drag n' Train Deployed Model\n\nTo run your model API:\n1. `pip install -r requirements.txt`\n2. `uvicorn app:app --reload`")
#     zip_buffer.seek(0)
#     return StreamingResponse(
#         zip_buffer,
#         media_type="application/zip",
#         headers={"Content-Disposition": "attachment;filename=drag_n_train_model_api.zip"}
#     )

import pandas as pd
import base64
import io
import os
import zipfile
import numpy as np
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from fastapi.responses import StreamingResponse

from fastapi.middleware.cors import CORSMiddleware
import google.generativeai as genai
from fpdf import FPDF
import joblib

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor
from sklearn.metrics import (r2_score, mean_squared_error, mean_absolute_error,
                             accuracy_score, f1_score, precision_score, recall_score, confusion_matrix)
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import seaborn as sns

import shap
import lime
import lime.lime_tabular

try:
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
    gemini_model = genai.GenerativeModel('gemini-2.5-flash')
    print("✅ Gemini API configured successfully.")
except Exception as e:
    print(f"⚠️ WARNING: Could not configure Gemini API. AI features will not work. Details: {e}")
    gemini_model = None

explanation_cache = {}
training_artifacts = {}

class ExplainResponse(BaseModel):
    explanation_plot: str
    explanation_type: str
    summary: Optional[str] = None

class Visualizations(BaseModel):
    confusion_matrix_plot: Optional[str] = None
    actual_vs_predicted_plot: Optional[str] = None
class FeatureImportance(BaseModel):
    feature: str
    importance: float
class TrainResponse(BaseModel):
    status: str
    model_name: str
    metrics: Dict[str, float]
    visualizations: Visualizations
    feature_importances: Optional[List[FeatureImportance]] = None
    features: List[str]
    generated_code: Optional[str] = None
class TrainRequest(BaseModel):
    dataset: List[Dict[str, Any]]
    target_column: str
    model_name: str
class SummarizeRequest(BaseModel):
    model_name: str
    target_column: str
    metrics: Dict[str, Any]
class ExplainRequest(BaseModel):
    term: str
class SuggestRequest(BaseModel):
    columns: List[str]
    objective: Optional[str] = None
class ProfileRequest(BaseModel):
    dataset: List[Dict[str, Any]]
class ProfileResponse(BaseModel):
    profile: Dict[str, Dict[str, Any]]
class PredictRequest(BaseModel):
    data: Dict[str, Any]
class ReportRequest(BaseModel):
    model_name: str
    metrics: Dict[str, float]
    summary: str
    visualizations: Dict[str, Optional[str]]

app = FastAPI(title="Drag n' Train AI API", version="8.2.0") # Version Bump!
app.add_middleware(CORSMiddleware, allow_origins=["http://localhost:8001", "https://drag-n-train.vercel.app"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

REGRESSION_MODELS = { "Linear Regression": LinearRegression, "Ridge": Ridge, "Lasso": Lasso, "Random Forest Regressor": RandomForestRegressor, "XGBoost Regressor": XGBRegressor }
CLASSIFICATION_MODELS = { "Logistic Regression": LogisticRegression, "Decision Tree Classifier": DecisionTreeClassifier, "K-Nearest Neighbors": KNeighborsClassifier, "Random Forest Classifier": RandomForestClassifier, "XGBoost Classifier": XGBClassifier }
ALL_MODELS = {**REGRESSION_MODELS, **CLASSIFICATION_MODELS}

def generate_python_code(model_name: str, target_column: str, features: List[str], model_class_str: str):
    # (No changes in this function)
    model_class_name = model_class_str.rsplit('.', 1)[1]
    evaluation_metric = "r2_score" if "Regressor" in model_name else "accuracy_score"
    return f"""# Code generated by Drag n' Train 🚀
import pandas as pd
from sklearn.model_selection import train_test_split
from {model_class_str.rsplit('.', 1)[0]} import {model_class_name}
from sklearn.metrics import {evaluation_metric}

# 1. Load your data (replace with your file path)
# df = pd.read_csv('your_data.csv')
# --- Assuming 'df' is your loaded DataFrame ---
# 2. Define Features (X) and Target (y)
features = {features}
target = '{target_column}'
X = df[features]
y = df[target]
# 3. Train the '{model_name}' model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = {model_class_name}()
model.fit(X_train, y_train)
# 4. Evaluate the model
predictions = model.predict(X_test)
score = {evaluation_metric}(y_test, predictions)
print(f"Model: {{model_name}}")
print(f"Score: {{score:.4f}}")
"""

def plot_to_base64(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches='tight')
    plt.close(fig)
    buf.seek(0)
    return base64.b64encode(buf.getvalue()).decode('utf-8')

@app.get("/models", summary="Get suggested models")
async def get_models(type: str):
    # (No changes in this function)
    if type == "regression": return {"models": list(REGRESSION_MODELS.keys())}
    if type == "classification": return {"models": list(CLASSIFICATION_MODELS.keys())}
    raise HTTPException(status_code=400, detail="Invalid problem type.")

@app.post("/train", response_model=TrainResponse, summary="Train model")
async def train_model(request: TrainRequest):
    # (No changes in this function)
    global training_artifacts
    if request.model_name not in ALL_MODELS: raise HTTPException(status_code=400, detail=f"Model '{request.model_name}' not found.")
    df = pd.DataFrame(request.dataset)
    X = df.drop(columns=[request.target_column]); y = df[request.target_column]
    features_list = X.columns.tolist()
    numeric_features = X.select_dtypes(include=['number']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
    problem_type = "regression" if request.model_name in REGRESSION_MODELS else "classification"

    preprocessor = ColumnTransformer(transformers=[('num', 'passthrough', numeric_features), ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)], remainder='passthrough')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_class = ALL_MODELS[request.model_name]
    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_class())])
    model_pipeline.fit(X_train, y_train); y_pred = model_pipeline.predict(X_test)

    model_instance = model_pipeline.named_steps['model']
    model_class_str = model_instance.__class__.__module__ + "." + model_instance.__class__.__name__
    generated_code = generate_python_code(request.model_name, request.target_column, features_list, model_class_str)
    metrics = {}; visuals = {}; feature_importances = None

    if problem_type == "regression":
        mse = mean_squared_error(y_test, y_pred)
        metrics["r2_score"] = round(r2_score(y_test, y_pred), 4)
        metrics["root_mean_squared_error"] = round(np.sqrt(mse), 4)
        metrics["mean_absolute_error"] = round(mean_absolute_error(y_test, y_pred), 4)
        fig, ax = plt.subplots(); ax.scatter(y_test, y_pred, alpha=0.7); ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)
        ax.set_xlabel("Actual Values"); ax.set_ylabel("Predicted Values"); ax.set_title("Actual vs. Predicted")
        visuals["actual_vs_predicted_plot"] = plot_to_base64(fig)
    elif problem_type == "classification":
        metrics["accuracy_score"] = round(accuracy_score(y_test, y_pred), 4); metrics["f1_score"] = round(f1_score(y_test, y_pred, average='weighted'), 4)
        metrics["precision"] = round(precision_score(y_test, y_pred, average='weighted', zero_division=0), 4); metrics["recall"] = round(recall_score(y_test, y_pred, average='weighted', zero_division=0), 4)
        cm = confusion_matrix(y_test, y_pred); labels = sorted(list(y_test.unique()))
        fig, ax = plt.subplots(); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=labels, yticklabels=labels)
        ax.set_xlabel('Predicted Labels'); ax.set_ylabel('True Labels'); ax.set_title('Confusion Matrix')
        visuals["confusion_matrix_plot"] = plot_to_base64(fig)

    try:
        ohe_features = model_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()
    except (AttributeError, ValueError):
        ohe_features = []
    feature_names_out = numeric_features + ohe_features

    if hasattr(model_pipeline.named_steps['model'], 'feature_importances_'):
        importances = model_pipeline.named_steps['model'].feature_importances_
        feature_importances = [{"feature": f, "importance": round(float(i), 4)} for f, i in sorted(zip(feature_names_out, importances), key=lambda x: x[1], reverse=True)]

    training_artifacts = {
        "model_pipeline": model_pipeline,
        "X_train": X_train,
        "X_test": X_test,
        "y_test": y_test,
        "feature_names_out": feature_names_out,
        "original_features": features_list,
        "categorical_features": categorical_features,
        "problem_type": problem_type,
        "class_names": [str(c) for c in sorted(y.unique())] if problem_type == "classification" else None
    }

    return TrainResponse(status="training_complete", model_name=request.model_name, metrics=metrics, visualizations=visuals, feature_importances=feature_importances, features=features_list, generated_code=generated_code)

# In main.py, replace the entire get_explanation function with this new one

@app.get("/explain", response_model=ExplainResponse, summary="Generate SHAP or LIME explanation plot")
async def get_explanation(type: str, instance_index: int = 0):
    if not training_artifacts:
        raise HTTPException(status_code=400, detail="No model has been trained yet. Please train a model first.")
    
    # --- New: Define a helper function to generate the summary ---
    async def generate_explanation_summary(explanation_data, explanation_type):
        if not gemini_model:
            return "Gemini API not configured. Cannot generate summary."

        target_column = training_artifacts.get("target_column", "the target value")
        
        # Prepare the data string for the prompt
        data_str = "\n".join([f"- {feature}: {round(value, 3)}" for feature, value in explanation_data])
        
        prompt = f"""
        As an expert data analyst, interpret the following model explanation and summarize it in plain English for a non-technical user.
        The model was trying to predict '{target_column}'.
        In this explanation, a positive value means the feature pushed the prediction HIGHER. A negative value means it pushed the prediction LOWER.

        Explanation Type: {explanation_type}
        Feature Impacts:
        {data_str}

        Instructions:
        1. Start by identifying the single most influential feature (positive or negative).
        2. Describe its effect on the prediction.
        3. Mention 1-2 other features that either supported or opposed this main effect.
        4. Conclude with a one-sentence summary of the story.
        5. Keep the entire response to 3-4 sentences. Be clear and concise.
        """
        try:
            response = await gemini_model.generate_content_async(prompt)
            return response.text
        except Exception as e:
            return f"Could not generate summary: {e}"

    model_pipeline = training_artifacts["model_pipeline"]
    X_train = training_artifacts["X_train"]
    X_test = training_artifacts["X_test"]
    problem_type = training_artifacts["problem_type"]
    summary = None # Initialize summary

    if type == "shap_summary":
        # (This part is mostly unchanged, just added summary generation)
        preprocessor = model_pipeline.named_steps['preprocessor']
        X_test_transformed = preprocessor.transform(X_test)
        explainer = shap.Explainer(model_pipeline.named_steps['model'], preprocessor.transform(X_train))
        shap_values = explainer(X_test_transformed)
        
        # Create plot
        shap.summary_plot(shap_values, X_test_transformed, feature_names=training_artifacts["feature_names_out"], show=False, plot_type="bar")
        fig = plt.gcf()
        b64_plot = plot_to_base64(fig)
        
        # Generate summary
        mean_abs_shap = np.abs(shap_values.values).mean(axis=0)
        shap_data_for_summary = sorted(zip(training_artifacts["feature_names_out"], mean_abs_shap), key=lambda x: x[1], reverse=True)[:10]
        summary = await generate_explanation_summary(shap_data_for_summary, "SHAP Summary (Global Importance)")
        
        return ExplainResponse(explanation_plot=b64_plot, explanation_type='shap_summary', summary=summary)

    elif type == "lime_instance":
        # (This part is mostly unchanged, just added summary generation)
        if instance_index >= len(X_test):
            raise HTTPException(status_code=400, detail=f"instance_index out of bounds. Max index is {len(X_test) - 1}.")
        
        original_features = training_artifacts["original_features"]
        categorical_features_names = training_artifacts["categorical_features"]
        categorical_features_indices = [original_features.index(col) for col in categorical_features_names]

        X_train_numeric = X_train.copy()
        categorical_names_map = {}
        mappings = {}
        for col_name in categorical_features_names:
            codes, uniques = pd.factorize(X_train[col_name])
            X_train_numeric[col_name] = codes
            col_idx = original_features.index(col_name)
            categorical_names_map[col_idx] = list(uniques.astype(str))
            mappings[col_name] = {label: code for code, label in enumerate(uniques)}

        def predict_fn_for_lime(numpy_array):
            df = pd.DataFrame(numpy_array, columns=original_features)
            for col_name in categorical_features_names:
                inv_map = {v: k for k, v in mappings[col_name].items()}
                df[col_name] = np.round(df[col_name]).astype(int).map(inv_map)
                df[col_name].fillna(X_train[col_name].mode()[0], inplace=True)
            if problem_type == "classification":
                return model_pipeline.predict_proba(df)
            else:
                return model_pipeline.predict(df)

        explainer = lime.lime_tabular.LimeTabularExplainer(
            training_data=X_train_numeric.to_numpy(), feature_names=original_features,
            class_names=training_artifacts["class_names"], categorical_features=categorical_features_indices,
            categorical_names=categorical_names_map, mode=problem_type)

        instance_to_explain_numeric = X_test.iloc[instance_index].copy()
        for col_name in categorical_features_names:
            instance_to_explain_numeric[col_name] = mappings[col_name].get(instance_to_explain_numeric[col_name])

        explanation = explainer.explain_instance(instance_to_explain_numeric.to_numpy(), predict_fn_for_lime, num_features=10)
        
        # Create plot
        fig = explanation.as_pyplot_figure()
        plt.tight_layout()
        b64_plot = plot_to_base64(fig)

        # Generate summary
        lime_data_for_summary = explanation.as_list()
        summary = await generate_explanation_summary(lime_data_for_summary, f"LIME Explanation for instance {instance_index}")

        return ExplainResponse(explanation_plot=b64_plot, explanation_type='lime_instance', summary=summary)

    else:
        raise HTTPException(status_code=400, detail="Invalid explanation type. Choose 'shap_summary' or 'lime_instance'.")
# --- NO FURTHER CHANGES BELOW THIS LINE ---

@app.post("/summarize", summary="Generate a simple summary of model results")
async def summarize_results(request: SummarizeRequest):
    if not gemini_model: return {"summary": "Gemini API not configured."}
    metrics_str = ", ".join([f"{k.replace('_', ' ')} of {v}" for k, v in request.metrics.items()])
    prompt = f"Analyze the performance of a '{request.model_name}' model that predicted '{request.target_column}'. The key metrics are: {metrics_str}. 1. Start with a one-sentence summary of the model's overall performance for a non-technical user. 2. Identify the single most important metric from the list and explain what it means in this specific context (e.g., what does an accuracy of 0.85 mean for predicting the target?). 3. Provide a short, actionable insight or next step. For example, 'This is a good starting point,' or 'This model seems to struggle with...' Keep the entire response to 3-4 sentences."
    try:
        response = gemini_model.generate_content(prompt); return {"summary": response.text}
    except Exception as e:
        return {"summary": f"Could not generate summary: {e}"}

@app.post("/explain", summary="Explain an ML term using Gemini")
async def explain_term(request: ExplainRequest):
    term = request.term.strip()
    if term in explanation_cache: return {"explanation": explanation_cache[term]}
    if not gemini_model: return {"explanation": "Gemini API not configured."}
    prompt = f"Explain the machine learning term '{term}' in two simple sentences for a complete beginner. Use a real-world analogy to make it clear (e.g., 'Precision is like a cautious archer...'). Do not use technical jargon."
    try:
        response = gemini_model.generate_content(prompt); explanation_cache[term] = response.text; return {"explanation": response.text}
    except Exception as e:
        return {"explanation": f"Could not get explanation: {e}"}

@app.post("/suggest_model", summary="Suggest a model based on dataset columns")
async def suggest_model(request: SuggestRequest):
    if not gemini_model: return {"suggestion": "Gemini API not configured."}
    columns_str = ", ".join(request.columns); available_models = ", ".join(list(ALL_MODELS.keys()))
    user_objective_prompt = f"The user's objective is: '{request.objective}'." if request.objective else "The user has not specified an objective."
    prompt = f"As an expert data scientist, analyze these dataset columns: {columns_str}. {user_objective_prompt} 1. **Task Recommendation:** Based on the columns and user objective, is this a 'Regression' or 'Classification' task? 2. **Target Suggestion:** Which column is the most likely target variable? 3. **Model Suggestions:** From this list of available models ({available_models}), which are the top 2 you would recommend trying first for this problem? 4. **Reasoning:** Provide a brief, simple justification for your recommendations. Format the response as a single, well-written paragraph."
    try:
        response = gemini_model.generate_content(prompt); return {"suggestion": response.text}
    except Exception as e:
        return {"suggestion": f"Could not generate suggestion: {e}"}

@app.post("/profile", response_model=ProfileResponse, summary="Generate a basic profile of the dataset")
async def profile_dataset(request: ProfileRequest):
    try:
        df = pd.DataFrame(request.dataset); profile = {}
        for col in df.columns:
            stats = {}; col_data = df[col].dropna()
            if pd.api.types.is_numeric_dtype(col_data):
                stats['type'] = 'Numeric'; stats['mean'] = float(round(col_data.mean(), 2)); stats['median'] = float(round(col_data.median(), 2))
                stats['std_dev'] = float(round(col_data.std(), 2)); stats['min'] = float(round(col_data.min(), 2)); stats['max'] = float(round(col_data.max(), 2))
            else:
                stats['type'] = 'Categorical'; stats['unique_values'] = int(col_data.nunique())
                stats['top_value'] = col_data.mode().iloc[0] if not col_data.mode().empty else 'N/A'
            profile[col] = stats
        return ProfileResponse(profile=profile)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Could not profile dataset: {e}")

@app.post("/predict", summary="Make a prediction using the last trained model")
async def predict(request: PredictRequest):
    if not training_artifacts: raise HTTPException(status_code=400, detail="No model has been trained yet.")
    try:
        input_df = pd.DataFrame([request.data], columns=training_artifacts["original_features"])
        prediction = training_artifacts["model_pipeline"].predict(input_df)
        prediction_value = prediction[0]
        if hasattr(prediction_value, 'item'): prediction_value = prediction_value.item()
        return {"prediction": prediction_value}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Could not make a prediction: {e}")

@app.post("/generate_report", summary="Generate a PDF report of the results")
async def generate_report(request: ReportRequest):
    try:
        pdf = FPDF(); pdf.add_page(); pdf.set_font("Helvetica", "B", 16)
        pdf.cell(0, 10, "Drag n' Train Analysis Report", 0, 1, 'C'); pdf.ln(10)
        pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, f"Model: {request.model_name}", 0, 1); pdf.ln(5)
        pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, "Analysis", 0, 1)
        pdf.set_font("Helvetica", "", 10); pdf.multi_cell(0, 5, request.summary.encode('latin-1', 'replace').decode('latin-1')); pdf.ln(5)
        pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, "Performance Metrics", 0, 1)
        pdf.set_font("Helvetica", "", 10); pdf.cell(95, 10, "Metric", 1, 0, 'C'); pdf.cell(95, 10, "Value", 1, 1, 'C')
        for key, val in request.metrics.items():
            pdf.cell(95, 10, key.replace('_', ' ').title(), 1, 0); pdf.cell(95, 10, str(val), 1, 1)
        pdf.ln(10)
        for name, b64_str in request.visualizations.items():
            if b64_str:
                pdf.add_page(); pdf.set_font("Helvetica", "B", 12); pdf.cell(0, 10, name.replace('_', ' ').title(), 0, 1)
                img_data = base64.b64decode(b64_str); img_file = io.BytesIO(img_data)
                pdf.image(img_file, x=10, y=None, w=190)
        pdf_buffer = io.BytesIO(); pdf_buffer.write(pdf.output()); pdf_buffer.seek(0)
        return StreamingResponse(pdf_buffer, media_type="application/pdf", headers={"Content-Disposition": "attachment;filename=drag_n_train_report.pdf"})
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate PDF report: {e}")

@app.get("/package_model", summary="Package the last trained model as a FastAPI microservice")
async def package_model():
    if not training_artifacts.get("model_pipeline"):
        raise HTTPException(status_code=400, detail="No model has been trained yet to package.")
    model_pipeline = training_artifacts["model_pipeline"]
    features = training_artifacts["original_features"]
    model_buffer = io.BytesIO()
    joblib.dump(model_pipeline, model_buffer)
    model_buffer.seek(0)
    app_template = f"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import pandas as pd
from typing import Dict, Any

model = joblib.load('model.pkl')
app = FastAPI(title="Drag n' Train Deployed Model")
class PredictionRequest(BaseModel):
    data: Dict[str, Any]
@app.post("/predict")
def predict(request: PredictionRequest):
    try:
        input_df = pd.DataFrame([request.data], columns={features})
        prediction = model.predict(input_df)
        prediction_value = prediction[0]
        if hasattr(prediction_value, 'item'):
            prediction_value = prediction_value.item()
        return {{"prediction": prediction_value}}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
@app.get("/")
def read_root():
    return {{"message": "Model API is running. POST to /predict with your data."}}
"""
    requirements = f"""fastapi\nuvicorn[standard]\nsklearn\npandas\njoblib\nxgboost\n"""
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("model.pkl", model_buffer.read())
        zf.writestr("app.py", app_template)
        zf.writestr("requirements.txt", requirements)
        zf.writestr("README.md", "# Drag n' Train Deployed Model\n\nTo run your model API:\n1. `pip install -r requirements.txt`\n2. `uvicorn app:app --reload`")
    zip_buffer.seek(0)
    return StreamingResponse(
        zip_buffer,
        media_type="application/zip",
        headers={"Content-Disposition": "attachment;filename=drag_n_train_model_api.zip"}
    )